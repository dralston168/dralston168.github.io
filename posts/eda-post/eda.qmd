---
title: EDA Notebook On Flooding Attacks in CAN networks (Assignment 2)
jupyter: python3
execute:
  enabled: false
---




### Importing necessary libraries and loading the dataset
This cell imports the `pandas` library and loads the dataset from a file located at `../data/dataset/Spark/Flooding_dataset_Spark.txt`. It also displays the shape, column names, and the first few rows of the dataset.

```{python}
import pandas as pd

df = pd.read_csv("../data/dataset/Spark/Flooding_dataset_Spark.txt")

print(f"Shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
df.head()
```

### Displaying dataset information
This cell provides an overview of the dataset, including the data types of each column and the number of non-null values.

```{python}
df.info()
```

### Generating descriptive statistics
This cell calculates and displays summary statistics for the numerical columns in the dataset, such as mean, standard deviation, minimum, and maximum values.

```{python}
df.describe()
```

### Checking for missing values
This cell calculates the number of missing values in each column of the dataset.

```{python}
df.isnull().sum()
```

### Visualizing the distribution of Column 8
This cell uses `matplotlib` and `seaborn` to create a histogram with a kernel density estimate (KDE) for the values in column `8`. The plot is saved as `distribution.png`.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

fig, ax = plt.subplots(figsize=(8, 5))
sns.histplot(df["8"], bins=50, kde=True, ax=ax)
ax.set_title("Distribution of Column 8")
ax.set_xlabel("Value")
ax.set_ylabel("Count")
fig.savefig("../figures/distribution.png", dpi=150, bbox_inches="tight")
plt.show()
```

![](distribution.png)

### Generating a correlation heatmap
This cell calculates the correlation matrix for numeric columns in the dataset and visualizes it using a heatmap. The plot is saved as `correlation_heatmap.png`.

```{python}
fig, ax = plt.subplots(figsize=(10, 8))
numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns
sns.heatmap(df[numeric_cols].corr(), annot=True, fmt=".2f", cmap="coolwarm", ax=ax)
ax.set_title("Feature Correlation Heatmap")
fig.savefig("../figures/correlation_heatmap.png", dpi=150, bbox_inches="tight")
plt.show()
```

![](correlation_heatmap.png)

### Creating a new categorical column and visualizing its distribution
This cell creates a new column `can_category` based on the values in the `04C1` column. It then visualizes the distribution of the new category using a count plot, which is saved as `class_distribution.png`.

```{python}
fig, ax = plt.subplots(figsize=(8, 6))

df["can_category"] = df["04C1"].apply(
    lambda x: 1 if x == "0000" else 0
)

sns.countplot(data=df, x="can_category", ax=ax)

ax.set_xticklabels(["Normal (not 0x000)", "Flooding (0x000)"])
ax.set_title("Message Count: CAN ID 0x000 vs Other CAN IDs")
ax.set_xlabel("CAN ID Category")
ax.set_ylabel("Number of Messages")

fig.savefig("../figures/class_distribution.png", dpi=150, bbox_inches="tight")
plt.show()
```

![](class_distribution.png)

### Visualizing data size distribution for different categories
This cell creates a violin plot to visualize the distribution of data sizes (`8`) for the two categories in the `can_category` column. The plot is saved as `violin_plot.png`.

```{python}
fig, ax = plt.subplots(figsize=(8, 6))

sns.violinplot(data=df, x="can_category", y="8", ax=ax)

ax.set_xticks([0, 1])
ax.set_xticklabels(["Normal", "Flooding (0x000)"])
ax.set_title("Data Size Distribution: Flooding vs Normal Messages")
ax.set_xlabel("CAN ID Category")
ax.set_ylabel("Data Size")

fig.savefig("../figures/violin_plot.png", dpi=150, bbox_inches="tight")
plt.show()
```

![](violin_plot.png)

### Extracting new features from string data
This cell creates two new features: `zero_count`, which counts the number of `00` bytes in the data columns of the CAN packet, and `rt_flag_numeric`, which is a numeric representation of the `rt_flag` column, representing 1 as 'T' and 0 as 'R'.

```{python}
#Get new numeric insights from string data

byte_cols = ["00", "CC", "80", "5E", "52", "08", "00.1", "00.2"]  

def count_zeros(row):
    size = int(row["8"])
    relevant_bytes = row[byte_cols[:size]]
    return (relevant_bytes == "00").sum()

df["zero_count"] = df.apply(count_zeros, axis=1)

def get_rt_flag(row):
    size = int(row["8"])
    if size<8:
        rt_col = byte_cols[size - 1 + 1]  
    else:
        rt_col = "R"
    return row[rt_col]

df["rt_flag"] = df.apply(get_rt_flag, axis=1)

df["rt_flag_numeric"] = (df["rt_flag"] == "T").astype(int)
```

### Creating a parallel coordinates plot
This cell creates a parallel coordinates plot to visualize the relationships between the features `8`, `zero_count`, `rt_flag_numeric`, and `can_category`. The plot is saved as `parallel_coordinates.png`.

```{python}
import matplotlib.pyplot as plt
from pandas.plotting import parallel_coordinates

plot_df = df[["8", "zero_count", "rt_flag_numeric", "can_category"]].copy()
plot_df = plot_df.sample(1000, random_state=42)
plot_df["can_category"] = plot_df["can_category"].map({0: "Normal", 1: "Flooding"})

fig, ax = plt.subplots(figsize=(10, 6))

parallel_coordinates(plot_df, "can_category",
                     color=["steelblue", "crimson"],
                     alpha=0.2,
                     ax=ax)

ax.set_title("Parallel Coordinates: Flooding vs Normal Messages")

fig.savefig("../figures/parallel_coordinates.png", dpi=150, bbox_inches="tight")
plt.show()
```

![](parallel_coordinates.png)

### Splitting the dataset into training and testing sets
This cell splits the dataset into training and testing sets using `train_test_split` from `sklearn`. The target variable is `can_category`, and only numeric columns are used as features.

```{python}
from sklearn.model_selection import train_test_split

X = df.drop("can_category", axis=1)
y = df["can_category"]

X = X.select_dtypes(include="number")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Train: {X_train.shape}, Test: {X_test.shape}")
```

### Training machine learning models
This cell trains two machine learning models: Logistic Regression and Random Forest Classifier, using the training data.

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
```

### Evaluating model performance
This cell evaluates the performance of the trained models (Logistic Regression and Random Forest) using the test dataset. It generates classification reports and confusion matrices for each model, and saves the confusion matrix plots as images

```{python}
from sklearn.metrics import classification_report, confusion_matrix

for name, model in [("Logistic Regression", lr), ("Random Forest", rf)]:
    y_pred = model.predict(X_test)
    print(f"\n{'='*40}")
    print(f"{name}")
    print(f"{'='*40}")
    print(classification_report(y_test, y_pred))
    
    fig, ax = plt.subplots(figsize=(6, 5))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_title(f"Confusion Matrix - {name}")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    fig.savefig(f"../figures/confusion_matrix_{name.lower().replace(' ', '_')}.png", 
                dpi=150, bbox_inches="tight")
    plt.show()
```

![](confusion_matrix_logistic_regression.png)
![](confusion_matrix_random_forest.png)