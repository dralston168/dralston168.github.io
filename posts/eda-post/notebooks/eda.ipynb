{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Exploratory Data Analysis: Survival IDS Dataset\"\n",
    "description: \"EDA and baseline ML models on the HCRL Survival IDS dataset.\"\n",
    "date: \"2026-02-27\"  # Use the date you completed the assignment\n",
    "categories: [eda, python, machine-learning]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Notebook On Flooding Attacks in CAN networks (Assignment 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing necessary libraries and loading the dataset\n",
    "This cell imports the `pandas` library and loads the dataset from a file located at `../data/dataset/Spark/Flooding_dataset_Spark.txt`. It also displays the shape, column names, and the first few rows of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset/Spark/Flooding_dataset_Spark.txt\")\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying dataset information\n",
    "This cell provides an overview of the dataset, including the data types of each column and the number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating descriptive statistics\n",
    "This cell calculates and displays summary statistics for the numerical columns in the dataset, such as mean, standard deviation, minimum, and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values\n",
    "This cell calculates the number of missing values in each column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Basic Data Cleaning\n",
    "This cell makes a new flag column where the flag for each packet exists in the same row and gets rid of the scattered flag encodings. Also, null data is replaced with a more representative string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"flag\"] = df.apply(lambda row: row[row.last_valid_index()] if row.last_valid_index() is not None else \"No data\", axis=1)\n",
    "df[\"flag\"] = df[\"flag\"].apply(lambda x: 1 if x == \"T\" else 0)\n",
    "cols = [c for c in df.columns if c != \"flag\"]\n",
    "df[cols] = df[cols].apply(\n",
    "    lambda row: row.mask(row.index == row.last_valid_index(), \"No data\"),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df.drop(columns=[\"R\"], inplace=True, errors=\"ignore\")\n",
    "df.drop(columns=[\"04C1\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "df.fillna(\"No data\", inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Data Cleaning\n",
    "this cell makes the data column names more representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    \"1513920093.615172\": \"Timestamp\",\n",
    "    \"8\": \"DLC\",\n",
    "    \"00\": \"Data[0]\",\n",
    "    \"CC\": \"Data[1]\",\n",
    "    \"80\": \"Data[2]\",\n",
    "    \"5E\": \"Data[3]\",\n",
    "    \"52\": \"Data[4]\",\n",
    "    \"08\": \"Data[5]\",\n",
    "    \"00.1\": \"Data[6]\",\n",
    "    \"00.2\": \"Data[7]\"\n",
    "}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the distribution of Column 8\n",
    "This cell uses `matplotlib` and `seaborn` to create a histogram with a kernel density estimate (KDE) for the values in column `DLC`. The plot is saved as `distribution.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.histplot(df[\"DLC\"], bins=50, kde=True, ax=ax)\n",
    "ax.set_title(\"Distribution of DLC\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "fig.savefig(\"../figures/distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a correlation heatmap\n",
    "This cell calculates the correlation matrix for numeric columns in the dataset and visualizes it using a heatmap. The plot is saved as `correlation_heatmap.png`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
    "ax.set_title(\"Feature Correlation Heatmap\")\n",
    "fig.savefig(\"../figures/correlation_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new categorical column and visualizing its distribution\n",
    "This cell visualizes the distribution of the new category using a count plot, which is saved as `class_distribution.png`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.countplot(data=df, x=\"flag\", ax=ax)\n",
    "\n",
    "ax.set_xticklabels([\"Normal (R)\", \"Flooding (T)\"])\n",
    "ax.set_title(\"Message Count: Normal vs Flooding\")\n",
    "ax.set_xlabel(\"CAN ID Category\")\n",
    "ax.set_ylabel(\"Number of Messages\")\n",
    "\n",
    "fig.savefig(\"../figures/class_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing data size distribution for different categories\n",
    "This cell creates a violin plot to visualize the distribution of data sizes (`DLC`) for the two categories in the `flag` column. The plot is saved as `violin_plot.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.violinplot(data=df, x=\"flag\", y=\"DLC\", ax=ax)\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels([\"Normal\", \"Flooding (0x000)\"])\n",
    "ax.set_title(\"Data Size Distribution: Flooding vs Normal Messages\")\n",
    "ax.set_xlabel(\"CAN ID Category\")\n",
    "ax.set_ylabel(\"Data Size\")\n",
    "\n",
    "fig.savefig(\"../figures/violin_plot.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting A More Specific Time Feature\n",
    "This collects timestamps within 100 ms and dislays the message count over these different time frames. This shows the pattern in message flow over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract message over time graph\n",
    "df[\"time_bucket\"] = df[\"Timestamp\"].apply(lambda x: int(x * 10) / 10)\n",
    "\n",
    "message_counts = df.groupby(\"time_bucket\").size().reset_index(name=\"message_count\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.lineplot(data=message_counts, x=\"time_bucket\", y=\"message_count\", ax=ax)\n",
    "\n",
    "ax.set_title(\"Message Count Over Time (100ms Windows)\")\n",
    "ax.set_xlabel(\"Time Window (100ms)\")\n",
    "ax.set_ylabel(\"Number of Messages\")\n",
    "\n",
    "fig.savefig(\"../figures/message_over_time.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking At High Traffic\n",
    "This specifically encodes each frame as a high traffic time frame or not. This helps with looking at a pattern between high message counts and injected messages. Specifcally, if messages in high traffic time frames are more likely to be flooding messgaes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_traffic = df.groupby(\"time_bucket\").size().reset_index(name=\"message_count\")\n",
    "high_traffic[\"high_traffic\"] = (high_traffic[\"message_count\"] > 300).astype(int)\n",
    "print(high_traffic.head())\n",
    "\n",
    "df = df.merge(high_traffic[[\"time_bucket\", \"high_traffic\"]], on=\"time_bucket\", how=\"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting new features from string data\n",
    "This cell looks at the uniqueness of the byte data in the data columns. Used to look for a pattern in variance of data between flooding and non-flooding messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [\"Data[0]\", \"Data[1]\", \"Data[2]\", \"Data[3]\", \"Data[4]\", \"Data[5]\", \n",
    "             \"Data[6]\", \"Data[7]\"]\n",
    "\n",
    "df[\"unique_count\"] = df[data_cols].apply(lambda row: row[row != \"No data\"].nunique(), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a parallel coordinates plot\n",
    "This cell creates a parallel coordinates plot to visualize the relationships between the features `DLC`, `high_traffic`, `unique_count`, and `flag`. The plot is saved as `parallel_coordinates.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "plot_df = df[[\"unique_count\", \"DLC\", \"high_traffic\", \"flag\"]].copy()\n",
    "plot_df = plot_df.sample(1000, random_state=42)\n",
    "plot_df[\"flag\"] = plot_df[\"flag\"].map({0: \"Normal\", 1: \"Flooding\"})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "parallel_coordinates(plot_df, \"flag\",\n",
    "                     color=[\"steelblue\", \"crimson\"],\n",
    "                     alpha=0.2,\n",
    "                     ax=ax)\n",
    "\n",
    "ax.set_title(\"Parallel Coordinates: Flooding vs Normal Messages\")\n",
    "\n",
    "fig.savefig(\"../figures/parallel_coordinates.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relooking at correlation\n",
    "This cell remakes a new correlation heatmap matrix with the new extracted features to determine their relavence in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relooking at correlation\n",
    "corr_df = df[[\"DLC\", \"unique_count\",\"high_traffic\", \"flag\"]].copy()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
    "ax.set_title(\"Feature Correlation Heatmap\")\n",
    "fig.savefig(\"../figures/correlation_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and testing sets\n",
    "This cell splits the dataset into training and testing sets using `train_test_split` from `sklearn`. The target variable is `flag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"flag\"])\n",
    "y = df[\"flag\"]\n",
    "\n",
    "X = X.select_dtypes(include=\"number\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Basic Preprocessing\n",
    "\n",
    "This is used to scale down the data specifically for logistic regression models to perform effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Fill NaNs\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training machine learning models\n",
    "This cell trains two machine learning models: Logistic Regression and Random Forest Classifier, using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance\n",
    "This cell evaluates the performance of the trained models (Logistic Regression and Random Forest) using the test dataset. It generates classification reports and confusion matrices for each model, and saves the confusion matrix plots as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "for name, model in [(\"Logistic Regression\", lr), (\"Random Forest\", rf)]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix - {name}\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    fig.savefig(f\"../figures/confusion_matrix_{name.lower().replace(' ', '_')}.png\", \n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Sanity\n",
    "This verifies that the model is learning meaningful patterns from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "print(f\"Dummy classifier score: {dummy.score(X_test, y_test):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
